---
title: "Lab 6: Machine Learning in Hydrology"
author: "Brandon Pizer ESS-330"
format:
  html:
    self-contained: true
---

**Lab Setup**
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(xgboost)

```
**Data Download**
```{r}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

```
**Basin Characteristics**
```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 

camels <- power_full_join(camels ,by = 'gauge_id')
```

**Question 1**

According to the pdf, zero_q_freq means frequency of days with Q = 0 mm/day shown as a percentage

**Exploratory Data Analysis**

```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```
**Question 2**

```{r}
library(ggplot2)
library(ggthemes)
library(patchwork)

# Create the two plots
aridity_plot <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(low = "grey", high = "red") +
  ggthemes::theme_map() +
  ggtitle("Aridity")

p_mean_plot <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "orange", high = "blue")  +
  ggthemes::theme_map() +
  ggtitle("Precipitation Mean") 


combined_plot <- aridity_plot + p_mean_plot  
print(combined_plot)

```

**Model Preparation**

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

```

```{r}

# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

```

```{r}

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

**Model Building**
```{r}
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

```{r}
# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

```{r}

# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)


```
```{r}
# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```
```{r}

test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)

metrics(test_data, truth = logQmean, estimate = lm_pred)

```

```{r}

ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")

```

```{r}
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients
```
```{r}

# From the base implementation
summary(lm_base)$coefficients

```

```{r}
#
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)
```

**Model Evaluation**
```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

**Switching it Up**
```{r}
library(baguette)
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 
```

**Predictions**
```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

**Model Evaluation: statistical and visual**
```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)
```

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

**A Workflowset Approach**

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)
```
```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

**Question 3: Your Turn**

```{r}
library(xgboost)
library(baguette)

xgb_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

nn_model <- bag_mlp() %>%
  set_engine("nnet") %>%
  set_mode("regression")

wf <- workflow_set(list(rec), list(lm_model, rf_model, xgb_model, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)

```

**Answer**
I would say that the neural net model is the better model moving forward because it has the highest root squared and lowest RMS error.

**Question 4**

**Data Splitting**
```{r}

set.seed(321)


camels_split_own <- initial_split(camels, prop = 0.8)

camels_train_own <- training(camels_split_own)
camels_test_own <- testing(camels_split_own)

camels_cv_own <- vfold_cv(camels_train_own, v = 10)

```

**Recipe**
```{r}

my_formula <- logQmean ~ runoff_ratio + baseflow_index

#I am choosing runoff_ratio and baseflow_index because I think that they could have correlation to mean daily discharge.

my_recipe <- recipe(my_formula, data = camels_train_own) %>%
  step_log(runoff_ratio, baseflow_index, offset = 1e-9) %>% 
  step_naomit(all_predictors(), all_outcomes())

```

**3 Models**
```{r}
### Define 3 models (25)

rf_model_own <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

lm_model_own <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")


xgb_model_own <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")
```

**Workflow_set**
```{r}
### workflow set () (15)

# Create a workflow_set object

wf_own <- workflow_set(list(rec), list(lm_model_own, rf_model_own, xgb_model_own)) %>%
  workflow_map('fit_resamples', resamples = camels_cv_own)
```


**Evaluation**
```{r}
autoplot(wf_own)

rank_results((wf_own), rank_metric = "rsq")

#Based on the cross-validation results, the random forest model appears to perform best because it has the highest rsq and lowest rmse.
```
**Answer**
Based on the cross-validation results, the random forest model appears to perform best because it has the highest rsq and lowest rmse.

**Extract and Evaluate**
```{r}
final_wf <- workflow() %>%
  add_recipe(my_recipe) %>%
  add_model(rf_model_own)

final_fit <- fit(final_wf, data = camels_train_own)

test_results <- augment(final_fit, new_data = camels_test_own)
metrics(test_results, truth = logQmean, estimate = .pred)

ggplot(test_results, aes(x = logQmean, y = .pred, color = baseflow_index + runoff_ratio)) +
  geom_point() +
  geom_abline(linetype = 2) +
  scale_color_viridis_c(option = "magma") +
  labs(
    title = "Observed vs Predicted Log Mean Flow (Test Data)",
    x = "Observed Log Mean Flow",
    y = "Predicted Log Mean Flow",
    color = "Predictors"
  ) +
  theme_linedraw()

```
**Answer**
Based off the results, I think that base flow and runoff ratio are good predictors of mean daily discharge. Looking at the rsq and rmse, they show that most of the variance is explained by the predictors. The Graph also represents a strong positive correlation too.