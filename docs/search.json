[
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "Lab Setup\n\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nData Download\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\nBasin Characteristics\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nQuestion 1\nAccording to the pdf, zero_q_freq means frequency of days with Q = 0 mm/day shown as a percentage\nExploratory Data Analysis\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\nQuestion 2\n\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\n# Create the two plots\naridity_plot &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"grey\", high = \"red\") +\n  ggthemes::theme_map() +\n  ggtitle(\"Aridity\")\n\np_mean_plot &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"orange\", high = \"blue\")  +\n  ggthemes::theme_map() +\n  ggtitle(\"Precipitation Mean\") \n\n\ncombined_plot &lt;- aridity_plot + p_mean_plot  \nprint(combined_plot)\n\n\n\n\n\n\n\n\nModel Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nModel Building\n\nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\nModel Evaluation\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nSwitching it Up\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\nModel Evaluation: statistical and visual\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nA Workflowset Approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\nQuestion 3: Your Turn\n\nlibrary(xgboost)\nlibrary(baguette)\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xgb_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.547  0.0309    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.787  0.0267    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0253    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0264    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nAnswer I would say that the neural net model is the better model moving forward because it has the highest root squared and lowest RMS error.\nQuestion 4\nData Splitting\n\nset.seed(321)\n\n\ncamels_split_own &lt;- initial_split(camels, prop = 0.8)\n\ncamels_train_own &lt;- training(camels_split_own)\ncamels_test_own &lt;- testing(camels_split_own)\n\ncamels_cv_own &lt;- vfold_cv(camels_train_own, v = 10)\n\nRecipe\n\nmy_formula &lt;- logQmean ~ runoff_ratio + baseflow_index\n\n#I am choosing runoff_ratio and baseflow_index because I think that they could have correlation to mean daily discharge.\n\nmy_recipe &lt;- recipe(my_formula, data = camels_train_own) %&gt;%\n  step_log(runoff_ratio, baseflow_index, offset = 1e-9) %&gt;% \n  step_naomit(all_predictors(), all_outcomes())\n\n3 Models\n\n### Define 3 models (25)\n\nrf_model_own &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nlm_model_own &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n\nxgb_model_own &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nWorkflow_set\n\n### workflow set () (15)\n\n# Create a workflow_set object\n\nwf_own &lt;- workflow_set(list(rec), list(lm_model_own, rf_model_own, xgb_model_own)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv_own)\n\nEvaluation\n\nautoplot(wf_own)\n\n\n\n\n\n\n\nrank_results((wf_own), rank_metric = \"rsq\")\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.571  0.0299    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.754  0.0219    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.587  0.0337    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.744  0.0192    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.609  0.0304    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.718  0.0282    10 recipe       boos…     3\n\n#Based on the cross-validation results, the random forest model appears to perform best because it has the highest rsq and lowest rmse.\n\nAnswer Based on the cross-validation results, the random forest model appears to perform best because it has the highest rsq and lowest rmse.\nExtract and Evaluate\n\nfinal_wf &lt;- workflow() %&gt;%\n  add_recipe(my_recipe) %&gt;%\n  add_model(rf_model_own)\n\nfinal_fit &lt;- fit(final_wf, data = camels_train_own)\n\ntest_results &lt;- augment(final_fit, new_data = camels_test_own)\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.352\n2 rsq     standard       0.916\n3 mae     standard       0.259\n\nggplot(test_results, aes(x = logQmean, y = .pred, color = baseflow_index + runoff_ratio)) +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  scale_color_viridis_c(option = \"magma\") +\n  labs(\n    title = \"Observed vs Predicted Log Mean Flow (Test Data)\",\n    x = \"Observed Log Mean Flow\",\n    y = \"Predicted Log Mean Flow\",\n    color = \"Predictors\"\n  ) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nAnswer Based off the results, I think that base flow and runoff ratio are good predictors of mean daily discharge. Looking at the rsq and rmse, they show that most of the variance is explained by the predictors. The Graph also represents a strong positive correlation too."
  },
  {
    "objectID": "hyperparameters.html",
    "href": "hyperparameters.html",
    "title": "Lab 8: Hyperparameters",
    "section": "",
    "text": "Initial Packages\n\nlibrary(tidyverse)\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.0     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(readr)\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.3\n\nlibrary(visdat)\n\nWarning: package 'visdat' was built under R version 4.4.3\n\nlibrary(ggpubr)\n\nWarning: package 'ggpubr' was built under R version 4.4.3\n\nlibrary(patchwork)\n\nWarning: package 'patchwork' was built under R version 4.4.3\n\nlibrary(glue)\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\n\nData Retrieval\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\nData Cleaning\n\nvis_dat(camels)\n\n\n\n\n\n\n\nvis_miss(camels, cluster = TRUE)\n\n\n\n\n\n\n\n\n\ncamels_clean &lt;- camels %&gt;%\n  drop_na()\n\n\nnames(camels_clean)\n\n [1] \"gauge_id\"             \"p_mean\"               \"pet_mean\"            \n [4] \"p_seasonality\"        \"frac_snow\"            \"aridity\"             \n [7] \"high_prec_freq\"       \"high_prec_dur\"        \"high_prec_timing\"    \n[10] \"low_prec_freq\"        \"low_prec_dur\"         \"low_prec_timing\"     \n[13] \"geol_1st_class\"       \"glim_1st_class_frac\"  \"geol_2nd_class\"      \n[16] \"glim_2nd_class_frac\"  \"carbonate_rocks_frac\" \"geol_porostiy\"       \n[19] \"geol_permeability\"    \"soil_depth_pelletier\" \"soil_depth_statsgo\"  \n[22] \"soil_porosity\"        \"soil_conductivity\"    \"max_water_content\"   \n[25] \"sand_frac\"            \"silt_frac\"            \"clay_frac\"           \n[28] \"water_frac\"           \"organic_frac\"         \"other_frac\"          \n[31] \"gauge_lat\"            \"gauge_lon\"            \"elev_mean\"           \n[34] \"slope_mean\"           \"area_gages2\"          \"area_geospa_fabric\"  \n[37] \"frac_forest\"          \"lai_max\"              \"lai_diff\"            \n[40] \"gvf_max\"              \"gvf_diff\"             \"dom_land_cover_frac\" \n[43] \"dom_land_cover\"       \"root_depth_50\"        \"root_depth_99\"       \n[46] \"q_mean\"               \"runoff_ratio\"         \"slope_fdc\"           \n[49] \"baseflow_index\"       \"stream_elas\"          \"q5\"                  \n[52] \"q95\"                  \"high_q_freq\"          \"high_q_dur\"          \n[55] \"low_q_freq\"           \"low_q_dur\"            \"zero_q_freq\"         \n[58] \"hfd_mean\"            \n\n\nData Splitting\n\nset.seed(10262004)\n\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\nRecipe\n\nlibrary(recipes)\n\nlibrary(recipes)\n\ncamels_rec &lt;- recipe(q_mean ~ ., data = camels_train) %&gt;%\n  update_role(gauge_lat, gauge_lon, new_role = \"ID\") %&gt;% \n  step_rm(gauge_id) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%  \n  step_impute_mean(all_numeric_predictors()) \n\n#I was having a ton of errors so this was the fix that chatgpt gave me\n\nResampling\n\nset.seed(10262004) \n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\nDefining Models\n\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"randomForest\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nwf_set &lt;- workflow_set(\n  list(camels_rec),  \n  list(nn_model, rf_model, xgb_model),  \n  cross = TRUE  \n)\n\nwf_results &lt;- wf_set %&gt;%\n  workflow_map(\n    \"fit_resamples\", \n    resamples = camels_cv,\n    control = control_grid(save_pred = TRUE)\n  )\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf Forest\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! There are new levels in `geol_1st_class`: \"Intermediate plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1\nThere were issues with some computations   A: x1   B: x1\n\n\nWarning: package 'randomForest' was built under R version 4.4.3\n\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf Forest\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x1\n→ B | warning: ! There are new levels in `geol_1st_class`: \"Intermediate plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1\nThere were issues with some computations   A: x1   B: x1\n\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf Forest\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n→ B | warning: ! There are new levels in `geol_1st_class`: \"Intermediate plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\nautoplot(wf_results)\n\n\n\n\n\n\n\n\nOut of the metrics, I am going to select XGBoost because it has a very high rsq and the lowest rmse. The model type is a boosted tree. The engine is xgboost. The mode is regression. I think its a good fit because the XGBoost model is robust, as it performs well with complex relationships present in the camels dataset.\nModel Testing\n\nxgb_model &lt;- boost_tree(\n  trees = tune(),           \n  tree_depth = tune()       \n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nxgb_workflow &lt;- workflow() %&gt;%\n  add_recipe(camels_rec) %&gt;%  \n  add_model(xgb_model)  \n\nxgb_grid &lt;- grid_regular(\n  trees(),\n  tree_depth(),\n  levels = 5\n)\n\n\nxgb_cv_resamples &lt;- vfold_cv(camels_train, v = 10)\n\n\nxgb_tune_results &lt;- tune_grid(\n  xgb_workflow,          \n  resamples = xgb_cv_resamples,\n  grid = xgb_grid,\n  metrics = metric_set(rmse, rsq)  \n)\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Intermediate plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x5\n\n\n→ B | warning: ! There are new levels in `geol_1st_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x5\nThere were issues with some computations   A: x5   B: x1\nThere were issues with some computations   A: x5   B: x2\nThere were issues with some computations   A: x5   B: x4\n→ C | warning: ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf Forest\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x5   B: x4\nThere were issues with some computations   A: x5   B: x5   C: x1\nThere were issues with some computations   A: x5   B: x5   C: x2\nThere were issues with some computations   A: x5   B: x5   C: x4\nThere were issues with some computations   A: x5   B: x5   C: x5\n\nxgb_tune_results\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics          .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [364/41]&gt; Fold01 &lt;tibble [50 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 2 &lt;split [364/41]&gt; Fold02 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [364/41]&gt; Fold03 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [364/41]&gt; Fold04 &lt;tibble [50 × 6]&gt; &lt;tibble [5 × 3]&gt;\n 5 &lt;split [364/41]&gt; Fold05 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [365/40]&gt; Fold06 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [365/40]&gt; Fold07 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [365/40]&gt; Fold08 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [365/40]&gt; Fold09 &lt;tibble [50 × 6]&gt; &lt;tibble [5 × 3]&gt;\n10 &lt;split [365/40]&gt; Fold10 &lt;tibble [50 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\nThere were issues with some computations:\n\n  - Warning(s) x5: ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf...\n  - Warning(s) x5: ! There are new levels in `geol_1st_class`: \"Intermediate plutoni...\n  - Warning(s) x5: ! There are new levels in `geol_1st_class`: \"Pyroclastics\". ℹ Con...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nHyperparameter Tuning\n\nxgb_tune_results &lt;- tune_grid(\n  xgb_workflow,          \n  resamples = xgb_cv_resamples,\n  grid = xgb_grid,\n  metrics = metric_set(rmse, rsq, mae)  \n)\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Intermediate plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x5\n\n\n→ B | warning: ! There are new levels in `geol_1st_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x5\nThere were issues with some computations   A: x5   B: x1\nThere were issues with some computations   A: x5   B: x3\nThere were issues with some computations   A: x5   B: x5\n→ C | warning: ! There are new levels in `dom_land_cover`: \" Evergreen Broadleaf Forest\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\nThere were issues with some computations   A: x5   B: x5\nThere were issues with some computations   A: x5   B: x5   C: x1\nThere were issues with some computations   A: x5   B: x5   C: x3\nThere were issues with some computations   A: x5   B: x5   C: x5\nThere were issues with some computations   A: x5   B: x5   C: x5\n\nautoplot(xgb_tune_results)\n\n\n\n\n\n\n\n\n\ntuned_metrics &lt;- collect_metrics(xgb_tune_results)\n\nhead(tuned_metrics)\n\n# A tibble: 6 × 8\n  trees tree_depth .metric .estimator  mean     n std_err .config              \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     1          1 mae     standard   0.947    10 0.0423  Preprocessor1_Model01\n2     1          1 rmse    standard   1.51     10 0.0588  Preprocessor1_Model01\n3     1          1 rsq     standard   0.742    10 0.0316  Preprocessor1_Model01\n4   500          1 mae     standard   0.145    10 0.00831 Preprocessor1_Model02\n5   500          1 rmse    standard   0.228    10 0.0163  Preprocessor1_Model02\n6   500          1 rsq     standard   0.983    10 0.00225 Preprocessor1_Model02\n\nshow_best(xgb_tune_results)\n\nWarning in show_best(xgb_tune_results): No value of `metric` was given; \"rmse\"\nwill be used.\n\n\n# A tibble: 5 × 8\n  trees tree_depth .metric .estimator  mean     n std_err .config              \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  2000          1 rmse    standard   0.218    10  0.0163 Preprocessor1_Model05\n2  1500          1 rmse    standard   0.218    10  0.0162 Preprocessor1_Model04\n3  1000          1 rmse    standard   0.220    10  0.0165 Preprocessor1_Model03\n4   500          1 rmse    standard   0.228    10  0.0163 Preprocessor1_Model02\n5   500          8 rmse    standard   0.230    10  0.0182 Preprocessor1_Model12\n\n\n\nbest_model_mae &lt;- show_best(xgb_tune_results, metric = \"mae\")\n\nbest_model_mae\n\n# A tibble: 5 × 8\n  trees tree_depth .metric .estimator  mean     n std_err .config              \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1  2000          4 mae     standard   0.122    10 0.00903 Preprocessor1_Model10\n2  1500          4 mae     standard   0.122    10 0.00903 Preprocessor1_Model09\n3   500          4 mae     standard   0.122    10 0.00903 Preprocessor1_Model07\n4  1000          4 mae     standard   0.122    10 0.00903 Preprocessor1_Model08\n5   500          8 mae     standard   0.125    10 0.00710 Preprocessor1_Model12\n\n\nThis is the combination of hyperparameters that gave the best value for mean absolute error. Hyperparameter Model Selection\n\nhp_best &lt;- select_best(xgb_tune_results, metric = \"mae\")\n\n\nhp_best\n\n# A tibble: 1 × 3\n  trees tree_depth .config              \n  &lt;int&gt;      &lt;int&gt; &lt;chr&gt;                \n1  2000          4 Preprocessor1_Model10\n\n\n\nxgb_final_wf &lt;- finalize_workflow(\n  xgb_workflow,\n  select_best(xgb_tune_results, metric = \"mae\")\n)\n\nxgb_final_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_rm()\n• step_dummy()\n• step_impute_mean()\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  trees = 2000\n  tree_depth = 4\n\nComputational engine: xgboost \n\n\n\nxgb_final_fit &lt;- last_fit(\n  xgb_final_wf,\n  split = camels_split\n)\n\n→ A | warning: ! There are new levels in `geol_1st_class`: \"Intermediate volcanic rocks\" and\n                 \"Basic plutonic rocks\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `geol_2nd_class`: \"Pyroclastics\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values., ! There are new levels in `dom_land_cover`: \" Barren or Sparsely Vegetated\".\n               ℹ Consider using step_novel() (`?recipes::step_novel()`) before `step_dummy()`\n                 to handle unseen values.\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1\n\n\n\n\ncollect_metrics(xgb_final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.243 Preprocessor1_Model1\n2 rsq     standard       0.972 Preprocessor1_Model1\n\n\nGraph\n\nxgb_final_predictions &lt;- collect_predictions(xgb_final_fit)\n\n\nxgb_final_fit %&gt;%\n  collect_predictions() %&gt;%\n  ggplot(aes(x = .pred, y = q_mean)) +\n  geom_point(alpha = 0.6, color = \"#0073C2FF\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray40\") +\n  labs(\n    title = \"Predicted vs Actual Mean Daily Discharge using Baseflow Index\",\n    x = \"Predicted Mean Daily Discharge\",\n    y = \"Actual Mean Daily Discharge\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nMapping\n\nfinal_fit_all &lt;- fit(xgb_final_wf, data = camels_clean)\n\n\n\nfull_preds &lt;- augment(final_fit_all, new_data = camels_clean)\n\nfull_preds &lt;- full_preds %&gt;%\n  mutate(residuals = (q_mean - .pred)^2)\n\n\n\nmap_pred &lt;- ggplot(full_preds, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"plasma\") +\n  coord_fixed(1.3) +\n  labs(\n    title = \"Predicted q_mean Across CONUS\",\n    color = \"Prediction\"\n  ) +\n  theme_minimal()\n\nmap_resid &lt;- ggplot(full_preds, aes(x = gauge_lon, y = gauge_lat, color = residuals)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"inferno\") +\n  coord_fixed(1.3) +\n  labs(\n    title = \"Residuals (Squared Error) Across CONUS\",\n    color = \"Residuals\"\n  ) +\n  theme_minimal()\n\n\nlibrary(patchwork)\n\nmap_pred + map_resid +\n  plot_annotation(title = \"Model Predictions and Residuals Across the US\")"
  }
]